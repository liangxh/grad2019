\chapter{意图识别技术}
\label{cha:tech}

\section{本章引论}

根据前一章中描述的研究框架，在本章中我们将针对每一个功能介绍对应的技术实现，其中包括文本预处理、词特征提取、分类算法、集成学习。对后续实验用中使用到的技术给出相对充分的说明，同时也会相关的技术给出概要的描述，以便于其他研究者在本论文未深入探索的方向作出拓展性的研究。

\section{文本预处理}

文本的预处理是所有面向文本的研究的第一步，其目的是为特征提取做好准备。良好的预处理策略可以在尽可能不掉失重要信息的情况下对样本数据进行简化，增加样本之间重复的模式，减轻模型对数据进行拟合的负担，同时更有效地找出数据之间的相互关系。错误的预处理策略会掉失具有区分能力的信息，甚至产生具有误导性的样本。对于不同的语言，由于其天然的性质不同，预处理的方法也会各异。本小节将针对社交媒体上的英语文本介绍几种预处理技术。

\subsection{分词}
\label{ssec:tokenization}

为了从文本提取词级别的特征，我们需要首先将句子切成多个词的序列。

虽然对于英语及大部分欧洲语言，空格隔开的字符必然属于不同的词组，但对于不以空格分隔词组的语言（如中文），分词的作用尤其重要。在某些情况下，分词的结果会影响对句子的理解，如将“乒乓球拍卖完了”切割成“乒乓球拍-卖-完-了”或“乒乓球-拍卖-完-了”，对主体应该是“乒乓球拍”还是“乒乓球”，动词应该是“卖”还是“拍卖”，仅凭字面意思无法确定发言者想表达的意思。特别地社交媒体平台上，新词不断的出现，要正确进行分词就有其独特的难点。而分词并不只针对语言中的单词，还针对标点符号或其他特征字符组成的有特殊语义或情感的字符组合，如现今社交媒体上普遍用多个字符拼接成颜文字，其中最常见的微笑的表情“:)”和伤心的表情“:(”，但如果在分词过程中把前者分割成“:”和“)”就会失去其所带的正向情感信息，这在短文本的情感识别中非常关键。

虽然利用空格和标点符号在大部分情况下可以完成对英语句子的分词，但在一些情况下，标点符号作用为词组的一部分而不是词组的分隔符，而在社交媒体上会有空格被省略的情况，以下是一些需要额外处理的情况\cite{jackson2007natural}\cite{mitkov2004oxford}。一是带句号的缩写，如“U.S.”，“.com”，句号应该作为词的一部分，或按句号切割“U.S.”将失去其语义。二是具有一定格式的带标点符号的词组，如电子邮箱地址（如example@email.com）、时间（如 Jan 6th、06/01/19）、电话号码（如(123)456-7890）、网页地址（如www.example.com）等，这些内容在大部分情况下可以被视为一个个体，我们更优先关注这个个体对应什么类型的事物，而不是其细节，譬如将一个句子中的电子邮箱地址或电话号码作替换并不会改变其情感的表达，但识别出一个字符串对应的事情类型，并在清楚它对意图识别没有关联的情况下对其忽略是有意义的。第三种情况是附属词，如“'t”对应“not”，只有正确识别“'”的作用才能识别出否定的意思，否则句子的意思将完全相反。值得注意的是，对社交媒体平台上的文本，除了以上在正规英语中会出现的情况，还有出现其他特殊情况，如“Y!E!S!”和“N!O!”。这需要对数据首先进行人工观察找出特殊的模式，再对语料库进行统计判断其出现频率，若出现频率较高则新增处理规则将对应模式做转换（如将“Y!E!S!”替换成“YES!!!”）或对问题无关的模式忽略（如国外的微博以“RE：”开头表示回复，并不包含任何情感）。

\subsection{拼写修正}

在处理较正式的文件时，我们一般可以默认其文本满足正规的语言语法，词汇基本都是正确的拼写或故意设计的新词（如杂志专栏作家首创针对英国退出欧盟首创单词“Brexit”）。不过在社交媒体上，拼写不符合传统英语的情况则非常普遍，这些情况可以分成三大类。

第一类是非刻意的拼意错误，由于社交媒体上的文本普遍是非正式的，用户不会刻意去保证文本的语言正确性，他们更关注于表达自己的想法、感情或其他意图。拼写修正技术一般基于统计的方法，对于一个不存在的词汇，尝试对其拼写进行有限次编辑，再评估编辑后的词在其上下文中出现的概率，最后选出可能性最高的一个词作替换。拼写修正技术已相当成熟地被应用于搜索引擎和手机键盘等应用中，读者可参考相关研究\cite{ahmed2009revised}\cite{nejja2015context}了解其细节。

第二种情况是网络上常用的代替用词，如以“thx”、“tnx”代替“thanks”，以“k”代替“ok”，以“cant”代替“can't”。虽然这些用法没有被认可为标准用法，但由于方便而在网络上被传播开成功网民们都能理解的用词。对此应对方法有两种，一是人工建立映射表，把代替用词映射到标准的英语词汇，优点在于可以直接引用标准词汇的语义信息，但缺点是需要人工参与，特别是在网络上新用词不断出现的情况下需要持续的更新。另一种是不做处理，利用机器学习方法从大型语料库中自动学习出它的语义，优点在于省去了人工的部分，缺点在于单词的出现很稀疏，对语料库的依赖很强。

第三种情况是语气加强，如“yeeeees”，“AMAZING”，但对于这一类情况处理的重点不只在于转换成标准用词，而是根据具体的应用问题判断是否要保留这个词存在语气加强的这个信息，譬如在情感识别中，语气加强一般提示了此处的情感需要注意。Baziotis等人\cite{baziotis2017semeval2}的做法是在单词前后添加标签示意，如“yeeeees”替换成“yes <enlongated>”，“AMAZING”替换成“amazing <allcaps>”。

\subsection{规范化}

在章节\ref{ssec:tokenization}中曾提及在一些特定的应用场景下，电子邮箱地址和日期等内容的细节其实对我们的研究任务并不起作用。对应地我们可以将内容一致但拼写不同的字符串转换成同一个词组来表示相同的概念，如Kouloumpis等人\cite{kouloumpis2011twitter}在处理微博文本时将网页地址和“@用户名”分别用同一个统一的字符串代替。更进一步地，Baziotis等人\cite{baziotis2017semeval2}在研究情感识别时，将表达同一种情感的不同表情符替换成对应的情感标签，如“:)))”替换成“<happy>”，“:-D”替换成“<laugh>”。但需要注意替换的内容是否会导致信息掉失，如Joshi等人\cite{joshi2015harnessing}在针对反讽识别的研究中指出一些反讽透过具体数字的对比来表达，在这种情况下将具体数字统一替换成表示数字的标签可能会导致预处理后的样本失去原本的反讽属性。

\section{特征提取}

数据经过预处理后，我们需要从获得的字符串或字符串序列中提取任务相关的特征，并进行量化方可作为分类器的数学模型的输入。本小节将介绍几种主流的文本特征提取技术。

\subsection{词嵌入}
\label{ssec:embedding}

词嵌入即捕足“词”的语义、语法、情感等信息并将其以向量表示。其中“词”对应预处理中分词得到的词，或对于字符级别的模型，“词”可以对应单个字符\cite{baziotis2018ntua}。主流的词嵌入算法分成两大类：基于人工神经网络的方法和基于矩阵分解的方法。

基于人工神经网络的方法假设句子中词之间存在某种上下文关系，并以神经网络建模学习，其中最具代表性的是CBOW和Skip-gram两种算法。CBOW的建模原理是句子中的一个“词”可以由它在句子中的前后有限个“词”决定，Skip-gram的建模原理则是句子中的“词”可以推测出它在句子中的前后有限个“词”。这两种算法都在自然语言处理中常用的词嵌入学习工具Word2vec\cite{mikolov2013efficient}中提供，又由于训练优质的词嵌入模型（指一组词到词向量的映射）需要收集大量的数据和耗费相当的时间，一些利用Word2vec训练好的词嵌入模型会作为公开的资源供研究员使用，以便于其他自然语言相关的研究，更方便于相互复现研究结果。其中较常用的是谷歌提供的模型\footnote{https://code.google.com/archive/p/word2vec/}，利用谷歌新闻作为语料训练出复盖约三百万个词的三百维向量集合。然而词向量所包含的信息和语料库紧密相关，所有也有一些研究会按照自己课题的特性自行收集相应的语料来训练词向量。如Baziotis等人\cite{baziotis2017semeval2}利用微博平台Twitter上收集的消息训练词向量模型并应用于面向微博的反讽识别，其词向量模型也被公开供其他研究者使用\footnote{https://github.com/cbaziotis/ntua-slp-semeval2018}。

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{img/word2vec.png}
  \caption{Word2vec提供的两个算法的模型，引自\cite{mikolov2013efficient}}
  \label{fig:word2vec}
\end{figure}

基于矩阵分解的方法假设一个表示词和词或词和文档之间关系的矩阵可以表示为由词向量组成的矩阵和其他矩阵的相乘，因此对该关系矩阵以对应方式进行矩阵分解即可以得到词向量。具代表性的方法是隐藏语义分析方法（Latent semantic analysis，LSA）\cite{deerwester1990indexing}，以词在各个文档中出现的频率得出词-文档矩阵，并假设其文档可以表示为词-词概念、词概念-文档概念、文档概念-文档三个矩阵相乘，利用奇异值分解（Singular Value Decomposition，SVD）得到词-词概念矩阵，取每一行作为对应词的词向量。另外还有Pennington等人\cite{pennington2014glove}提出的GloVe算法，该算法根据不同词在语料中出现在各自上下文窗口中的频率得出词和词之间的关系矩阵，再分解成两个词-词概念矩阵的相乘。其研究结果显示在部分自然语言处理任务中，采用GloVe较Word2vec及另外几种词嵌入技术得出的词向量能达到更好的效果。他们同样公开了多个利用不同语料库训练好的词向量模型供其他研究者使用\footnote{https://nlp.stanford.edu/projects/glove/}。

\subsection{词汇特征} % Lexical Feature 

词汇特征表示文本中一元或多元语法的分布情况，不考虑词的具体意思。譬如常用的词袋模型（Bag of words, BOW），其将一段文本映射到 一个固定长度的向量，向量上每一维对应一个一元或多元语法的某种属性，譬如它是否在文本中出现（即独热编码），或它在文本中出现的频率，或其词频-逆文档频度(Term Frequency - Inverse Document Frequency，TF-IDF)。或更简地以单词数量或字母数量作为一维的特征。

\subsection{句法特征} % Syntactic features 

句法特征表示文本中各个词在句子的语法作用。常见做法是先对文本进行词性（Part-of-speech，POS）标注和依赖树分析，再基于标注提取特征，如形容词的数量、副词的数量，或以标注代替单词后采用词袋模型得出固定长度的特征向量。一些研究会针对性地手工设计句法特征，这要求研究者对语法的了解和挖掘它对研究的问题是否存在关联性，但特征的作用更直观，解释性更强。

\subsection{语义特征} % Semantic features

语义特征基于文本的字面意思，表示文本表达的内容或内容的个别属性‌（如主题分布），这更接近于人们透过理解句子内容来进行识别。常见技术譬如基于文本中各个词所对应词向量计算句子的表示向量，或利用词向量计算句子中各单词和情感词的相似度来间接评估词的情感极性，或利用聚类算法对语料库的单词进行聚类来获得单词的隐藏语义类别，再计算句子中各类别单词的分布。

\section{模型}

本小节将给出搭建意图识别系统相关的模型。目前用于意图识别的主流模型可以分成两大类：传统机器学习和人工神经网络。我们会分别对这两类模型中具代表性的例子给出介绍，然后再介绍集成学习方法，以在有限个模型的基础上搭建集成识别系统，达到比单个模型更高的性能。

\subsection{传统机器学习}

支持向量机（Support Vector Machine, SVM）\cite{cortes1995support} 由Cortes和Vapnik在1995年提出，是目前最常用的传统机器学习算法之一。在对二分类问题的建模过程中，每个训练样本被映射到一个空间中的一个点，而支持向量机尝试把找出一个边缘把空间分成两个子空间（如以直线分割二维空间，以平面分割三维空间），每一个子空间对应一个类，使得训练样本落在对应类别的子空间中，并且各样本到边缘的距离最大。对于多分类问题，主流做法是把原问题拆解成多个二分类问题。后随着核函数的引入，将支持向量机拓展成非线性的数学模型，大大加强了其建模能力。除了分类问题，支持向量机同样可以应用于回归问题，Drucker等人\cite{drucker1997support}则提出了支持向量机的回归模型。支持向量机最大的限制之一是只接受固定长度的向量作为输入，要求时先从输入数据提取出固定长度的特征向量，换言之基于支持向量机的识别系统非常依赖对特征的提取。

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{img/svm.pdf}
  \caption{支持向量机在处理二类问题的示意图。其中黑点和白点分别对应两个类别的样本，$L_1$不能对两类样本作区分，$L_2$和$L_3$均成功区分两个类别的样本，但$L_3$和两个类别样本的距离更大，为更优解}
  \label{fig:svm}
\end{figure}

决策树（Decision Tree）是另一种常见的机器学习方法。对于分类问题，决策树基于训练数据建立一个树状模型，其中每个非子叶结点对应一个决策，叶子结点对应要识别的类别。在识别阶段，识别过程从根结点开始，经过每个非叶子结点根据输入数据得出决策结果，不同的决策结果会对应下一级的一个结点，如此不断前进直到到达叶子结点，以其对应类别作为分类结果。和支持向量机一样，决策树的输入为固定长度的向量，要求对应的特征提取。现今的研究更多地会使用由多个决策树组成的随机森林（Random Forest）而非单个决策树作为分类器，随机森林属于集成学习一类，将在后续章节\ref{ssec:intro_ensemble_learning}中介绍。

\subsection{人工神经网络}

人工神经网络源自仿生学，以数学模型模拟生物神经网络的响应过程和学习过程。Rosenblatt提出的感知器\cite{rosenblatt1958perceptron}是最早的人工神经网络，也是现在人工神经网络的基本单元（又称为神经元）。Rumelhart等人\cite{rumelhart1985learning}在后来提出了一种多层前馈神经网络，BP网络，由多层的感知器和激励函数反复堆叠而成。更重要的是他们提出了它的学习算法，反向传播算法（Backpropagation，BP），当网络的输出结果和预期结果不同，透过构造损失函数计算偏差，再根据损失函数和偏差计算出神经元权重的梯度，其对应了使误差扩大的权重方向，以其反方向作为权重修改的依据。同时将该层输入的修改量作为前一层输出的偏差，逐层重复修改权重，整个过程重复迭代直到误差满足一定条件时学习过程结束。反向传播算法是目前人工神经网络模型训练中的核心算法，但随着网络层数增加，梯度以指数级别增大或缩小，会导致梯度大小过大或近似于零的情况，即梯度爆炸和梯度消失。直到Hinton等人\cite{hinton2006fast}提出深度信念网络和无监督逐层训练的策略，其核心思想是在每一轮训练只调整网络中一层的权重，透过固定前一层的输出作为本层的输入，避开了梯度的多次传播。如此网络逐层经过调整后，再对整个网络进行权重调重，理论上此时的梯度较小，避免了梯度爆炸的出现。这使得现在复杂人工神经网络的学习成为了可能，深度学习正式进入高速发展的阶段。接下来我们将对主流的人工神经网络单元作出介绍。

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{img/perceptron.png}
  \caption{生物的神经元与感知器的模型示意图，引自\cite{basheer2000artificial}}
  \label{fig:perceptron}
\end{figure}


% 人工神经网络对每一个输入进行预测的中间结果可被认为是该输入的隐藏特征
% 利用不同标注的数据集训练的人工神经网络可以用作对应类型的特征提取，如
%  SemEval2014 Task 9: 标注为文本的正负中性情感
%  SemEval2018 Task 1: 标注为文本中是否分别包含了11种情感

% Gated Recurrent Unit, GRU 
% 长短期记忆网络 Long Short-Term Memory, LSTM
% 双向长短期记忆网络 Bidirectional Long Short-Term Memory, BLSTM
% 卷积神经网络 Convolutional Neural Network CNN

\subsection{集成学习算法}
\label{ssec:intro_ensemble_learning}

pass

% 后融合
%   结合多个子系统的预测结果，根据特定策略得出新的预测结果

%   多数投票 Majority Voting / Hard Voting
%     每个模型分别给出预测标签
%     取最多模型预测的标签作为最终预测结果

%   加权多数投票 Weighted Majority Vote
%     每个模型分别给出预测标签
%     对这些模型的预测标签进行加权投票，取投票最多的标签作为预测结果

%   加权平均概率投票 Soft Voting
%     每个模型分别给出各个标签的预测概率
%     对每个模型的预测概率进行加权不均，取概率最高的标签作为预测结果


\section{本章小结}

pass
