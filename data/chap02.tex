\chapter{研究框架与相关技术}
\label{cha:problem_framework}

\section{本章引论}

随着互联网上不同类型的平台出现，人们每天在各种平台上产生着各种各样的行为和发言。如现在微博平台上会对热门时事设置井号标签，人们透过加上对应标签来表达对该事件的想法，以及透过对这些发言进行赞点、分享或评论来表达支持或反驳。利用井号标签收集数据并进行情感分析可以得知网民对该事件的舆论方向。在其他平台上的各种行为记录同样可能有挖掘其情感的价值，情感识别的应用场景也变得多种多样，但即使数据的内容和结构不同，问题的本质是相似的。

本章的内容安排如下。在章节\ref{sec:global_problem_analysis}中，我们将首先针对情感识别进行分析，并给出统一的形式化表示。基于该形式化表示，在章节\ref{sec:global_framework}中，我们再进一步提出一个面向社交媒体文本的情感识别研究框架。从原始文本的输入到最终识别目标的输出，理清其中每个步骤的功能和目的，给出一个完整识别系统的设计方案，为解决后续章节中研究的问题准备一个统一的切入点。

\section{问题分析}
\label{sec:global_problem_analysis}

本小节中，我们将对情感识别中涉及的各个元素作出分析，并给出统一的形式化表示来描述他们的相互关系。

不同情感识别问题中都有要被识别的情感倾向$C$。如Tang等人\cite{tang2015learning}的情感极性识别研究，$C$对应需要五级的情感极性。在刘丹丹等人\cite{刘丹丹2015基于}的微博情感分类研究中，$C$对应喜好、安乐、惊奇、厌恶、悲哀、愤恨、恐惧。邓钊等人\cite{2015面向微博的中文反语识别研究}的中文反语识别研究中，$C$则对应是否包含反讽。

其次是研究主体$T$，它是行为发起者$S$的行为记录，可以认为他表达想法和情感的载体。另外一些场景下会有背景信息$B$，对应所有有助于正确理解$T$的信息。譬如要研究讨论区上帖子的情感倾向，那么$T$就是帖子的内容，包括其中的文本内容、图片、文件附件等，$S$则是帖子的发布者。而帖子所在讨论区的类型有助于定位帖子对应的领域，发布者的发布历史显示发布者的一些态度倾向，帖子下的评论从侧表反映主帖的内容，这些就是背景信息$B$，都可能是理解帖子内容的提示。又以Zahiri等人\cite{Zahiri2017Emotion}对电视剧台词的情感识别研究为例，那么$T$指电视剧中的台词，$S$则是发出这段台词的对应角色，$B$则是台词的上文，其中包含其他角色正在谈论的内容，这些角色和发言者的关系是什么，说话的氛围如何等等，都能对台词的内容有更明确的定位。

情感识别假设对于任意一个研究样本$t \in T$，在给定背景信息$b \in B$（或某些情况下假设与背景信息无关），其承载想法或情感必然存在对应的倾向$c \in C$。情感识别首先要从样本主体和背景信息中分别提取出与识别目标相关的信息$f_t$和$f_b$，即需要提出两个映射函数，$F_T$和$F_B$，满足 $f_t=F_T(t)$和$f_b=F_B(b)$。再进一步根据相关信息识别出其想法或情感倾向，即找出一个映射关系$F_C$，使得 $c=F_C(f_t, f_b)=F_C(F_T(t), F_B(b))$。

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{img/framework.pdf}
  \caption{情感识别的研究框架}
  \label{fig:framework}
\end{figure}

\section{研究框架}
\label{sec:global_framework}

在本小节，我们将根据前述的形式化表示给出一个情感识别的研究框架。由于在本论文的后续研究均面向英语社交媒体文本的分类问题，我们会针对此研究方法对框架进行细化，从原始文本的输入到最终识别目标的输出，理清其中每个步骤的功能和目的，给出一个集成系统的设计框架。针对不同的应用的场景，在后续实验章节中将给出具体的实现方案。

\subsection{框架入口}

图~\ref{fig:framework}显示本文面向情感识别的研究框架。整个框架的输入是实验的数据集，数据集的每个样本包含三个元素，分别是研究的主体、背景信息和情感的倾向。参考前一小节的形式化表示，一个样本可以表示为 $<t, b, c> \in T \times B \times C$。不论是训练数据或测试数据，他们都采用相同的数据预处理技术和特征提取技术来获取样本的特征，以用作分类器的输入，即透过$F_T$和$F_B$分别获取$f_t=F_T(t)$和$f_b=F_B(b)$。

\subsection{基础分类器}

在特征提取完成后，识别模型的输入就准备好了。接下来首先对基础分类器进行研究和分析，比较不同模型和不同参数在对应问题上的整体性能。对于机器学习方法，在模型的训练过程中，需要将训练数据会分成训练集和验证集两个部分。训练集用于调整模型的参数，根据预先设计的损失函数计算模型当前的预测结果和实际标签的偏差，透过微分计算出参数的调整方向，并以一定的学习步长逐步迭代。由于以上训练过程在数学逻辑上是以达到训练集上的最优解来调整参数，会出现对训练样本过拟合的问题，因此会采用验证集来评估模型对训练集以外的样本的识别性能。在每一轮参数调整后计算当前模型在验证集上的性能指标，最后选择其中最优的一轮参数作为该次模型训练最终的参数。

除了分析不同模型在对应问题上的整体性能，透过对被分类器错误识别的样本进行观察，我们需要分析单个分类器的性能局限在哪，分类器对哪些类别的样本有更高的正确率或召回率，哪些样本之间容易出现混淆，或者在输入数据当中是否存在一些有关键信息的特征没有被成功提取。特别地情感识别的研究课题中，我们会关注数据集标签的正确性。在实验的过程中我们假设标签是正确，但有研究指出自动标注会引入噪声\cite{littlestone1988learning}，人工标注则难以避免地存在主观性。虽然对不同算法之间的性能比较影响不大，但对于技术水平的评估有其分析的价值。

\subsection{集成识别系统}

经过对基础分类器的分析后，我们对不同模型的识别性能有了深入的了解。为了进一步达到更好的性能，
下一步是基于基础分类器研究集成识别系统的策略和设计。我们提出了一种集成系统的设计方案，结合多个模型相同的分类器的预测结果来作出最终判断，旨在只使用一种模型的基础上尽可能提升识别的性能，或针对特定的性能指标对整个系统的识别倾向进行调整。参考决策树，其识别过程可以认为是一系列的决策，每一步决策都建立在前一步的决策结果上，并且只关注原始问题中的子问题。类似地，我们提出的集成系统经过多次决策来对样本进行情感识别，第一步先由一个分类器或多个分类器的预测结果融合得到一个基础的预测结果，接下来的每一步需要先对前一步的混淆矩阵进行分析，根据哪些类别的样本被误判对指标的影响较大，设定一个子识别任务，从训练数据中筛选出相关的子集来训练新的基础分类器，得出针对该子识别任务的决策结果，决定是否修改上一步中的预测结果。这一方法的原理在于相同的模型在相同配置下的拟合能力是有限的，而子识别任务是原始问题的简化，模型可以专注于拟合一个局部问题，并对前一步的预测结果提出修改意见，起着补丁的作用。具体方法将在后续实验章节中给出详细说明。

同样地，我们需要分析集成系统的性能。在测试数据上，观察系统经过每一轮决策调整之后的识别性能变化，验证系统框架的设计是否满足假设。另外深入观察混淆矩阵的变化，分析各个子识别任务有效的原因，判断其适用和不适用的场景。

\section{相关技術}

\subsection{文本预处理}
\label{ssec:text_preprocess}

文本的预处理是所有面向文本的研究的第一步，其目的是为特征提取做好准备。良好的预处理策略可以在尽可能不掉失重要信息的情况下对样本数据进行简化，增加样本之间重复的模式，减轻模型对数据进行拟合的负担，同时更有效地找出数据之间的相互关系。错误的预处理策略会掉失具有区分能力的信息，甚至产生具有误导性的样本。对于不同的语言，由于其天然的性质不同，预处理的方法也会各异。本小节将针对社交媒体上的英语文本介绍几种预处理技术。

\subsubsection{分词}
\label{sssec:tokenization}

为了从文本提取词级别的特征，我们需要首先将句子切成多个词的序列。

虽然对于英语及大部分欧洲语言，空格隔开的字符必然属于不同的词组，但对于不以空格分隔词组的语言（如中文），分词的作用尤其重要。在某些情况下，分词的结果会影响对句子的理解，如将“乒乓球拍卖完了”切割成“乒乓球拍-卖-完-了”或“乒乓球-拍卖-完-了”，对主体应该是“乒乓球拍”还是“乒乓球”，动词应该是“卖”还是“拍卖”，仅凭字面意思无法确定发言者想表达的意思。特别地社交媒体平台上，新词不断的出现，要正确进行分词就有其独特的难点。而分词并不只针对语言中的单词，还针对标点符号或其他特征字符组成的有特殊语义或情感的字符组合，如现今社交媒体上普遍用多个字符拼接成颜文字，其中最常见的微笑的表情“:)”和伤心的表情“:(”，但如果在分词过程中把前者分割成“:”和“)”就会失去其所带的正向情感信息，这在短文本的情感识别中非常关键。

虽然利用空格和标点符号在大部分情况下可以完成对英语句子的分词，但在一些情况下，标点符号作用为词组的一部分而不是词组的分隔符，而在社交媒体上会有空格被省略的情况，以下是一些需要额外处理的情况\cite{jackson2007natural}\cite{mitkov2004oxford}。一是带句号的缩写，如“U.S.”、“.com”，句号应该作为词的一部分，或按句号切割“U.S.”将失去其语义。二是具有一定格式的带标点符号的词组，如电子邮箱地址（如example@email.com）、时间（如 Jan 6th、06/01/19）、电话号码（如(123)456-7890）、网页地址（如www.example.com）等，这些内容在大部分情况下可以被视为一个个体，我们更优先关注这个个体对应什么类型的事物，而不是其细节，譬如将一个句子中的电子邮箱地址或电话号码作替换并不会改变其情感的表达，但识别出一个字符串对应的事情类型，并在清楚它对情感识别没有关联的情况下对其忽略是有意义的。第三种情况是附属词，如“'t”对应“not”，只有正确识别“'”的作用才能识别出否定的意思，否则句子的意思将完全相反。值得注意的是，对社交媒体平台上的文本，除了以上在正规英语中会出现的情况，还有出现其他特殊情况，如“Y!E!S!”和“N!O!”。这需要对数据首先进行人工观察找出特殊的模式，再对语料库进行统计判断其出现频率，若出现频率较高则新增处理规则将对应模式做转换（如将“Y!E!S!”替换成“YES!!!”）或对问题无关的模式忽略（如国外的微博以“RE：”开头表示回复，并不包含任何情感）。

\subsubsection{拼写修正}

在处理较正式的文件时，我们一般可以默认其文本满足正规的语言语法，词汇基本都是正确的拼写或故意设计的新词（如杂志专栏作家首创针对英国退出欧盟首创单词“Brexit”）。不过在社交媒体上，拼写不符合传统英语的情况则非常普遍，这些情况可以分成三大类。

第一类是非刻意的拼意错误，由于社交媒体上的文本普遍是非正式的，用户不会刻意去保证文本的语言正确性，他们更关注于表达自己的想法、感情或其他情感。拼写修正技术一般基于统计的方法，对于一个不存在的词汇，尝试对其拼写进行有限次编辑，再评估编辑后的词在其上下文中出现的概率，最后选出可能性最高的一个词作替换。拼写修正技术已相当成熟地被应用于搜索引擎和手机键盘等应用中，读者可参考相关研究\cite{ahmed2009revised}\cite{nejja2015context}了解其细节。

第二种情况是网络上常用的代替用词，如以“thx”、“tnx”代替“thanks”，以“k”代替“ok”，以“cant”代替“can't”。虽然这些用法没有被认可为标准用法，但由于方便而在网络上被传播开成功网民们都能理解的用词。对此应对方法有两种，一是人工建立映射表，把代替用词映射到标准的英语词汇，优点在于可以直接引用标准词汇的语义信息，但缺点是需要人工参与，特别是在网络上新用词不断出现的情况下需要持续的更新。另一种是不做处理，利用机器学习方法从大型语料库中自动学习出它的语义，优点在于省去了人工的部分，缺点在于单词的出现很稀疏，对语料库的依赖很强。

第三种情况是语气加强，如“yeeeees”、“AMAZING”，但对于这一类情况处理的重点不只在于转换成标准用词，而是根据具体的应用问题判断是否要保留这个词存在语气加强的这个信息，譬如在情感识别中，语气加强一般提示了此处的情感需要注意。Baziotis等人\cite{baziotis2017semeval2}的做法是在单词前后添加标签示意，如“yeeeees”替换成“yes <enlongated>”，“AMAZING”替换成“amazing <allcaps>”。

\subsubsection{规范化}

在章节\ref{sssec:tokenization}中曾提及在一些特定的应用场景下，电子邮箱地址和日期等内容的细节其实对我们的研究任务并不起作用。对应地我们可以将内容一致但拼写不同的字符串转换成同一个词组来表示相同的概念，如Kouloumpis等人\cite{kouloumpis2011twitter}在处理微博文本时将网页地址和“@用户名”分别用同一个统一的字符串代替。更进一步地，Baziotis等人\cite{baziotis2017semeval2}在研究情感识别时，将表达同一种情感的不同表情符替换成对应的情感标签，如“:)))”替换成“<happy>”，“:-D”替换成“<laugh>”。但需要注意替换的内容是否会导致信息掉失，如Joshi等人\cite{joshi2015harnessing}在针对反讽识别的研究中指出一些反讽透过具体数字的对比来表达，在这种情况下将具体数字统一替换成表示数字的标签可能会导致预处理后的样本失去原本的反讽属性。

\subsection{特征提取}
\label{ssec:feature_extraction}

数据经过预处理后，我们需要从获得的字符串或字符串序列中提取任务相关的特征，并进行量化方可作为分类器的数学模型的输入。本小节将介绍几种主流的文本特征提取技术。

\subsubsection{词嵌入}
\label{sssec:embedding}

词嵌入即捕足“词”的语义、语法、情感等信息并将其以向量表示。其中“词”对应预处理中分词得到的词，或对于字符级别的模型，“词”可以对应单个字符\cite{baziotis2018ntua}。主流的词嵌入算法分成两大类：基于人工神经网络的方法和基于矩阵分解的方法。

基于人工神经网络的方法假设句子中词之间存在某种上下文关系，并以神经网络建模学习，其中最具代表性的是CBOW和Skip-gram两种算法。CBOW的建模原理是句子中的一个“词”可以由它在句子中的前后有限个“词”决定，Skip-gram的建模原理则是句子中的“词”可以推测出它在句子中的前后有限个“词”。这两种算法都在自然语言处理中常用的词嵌入学习工具Word2vec\cite{mikolov2013efficient}中提供，又由于训练优质的词嵌入模型（指一组词到词向量的映射）需要收集大量的数据和耗费相当的时间，一些利用Word2vec训练好的词嵌入模型会作为公开的资源供研究员使用，以便于其他自然语言相关的研究，更方便于相互复现研究结果。其中较常用的是谷歌提供的模型\footnote{https://code.google.com/archive/p/word2vec/}，利用谷歌新闻作为语料训练出复盖约三百万个词的三百维向量集合。然而词向量所包含的信息和语料库紧密相关，所有也有一些研究会按照自己课题的特性自行收集相应的语料来训练词向量。如Baziotis等人\cite{baziotis2017semeval2}利用微博平台Twitter上收集的消息训练词向量模型并应用于面向微博的反讽识别，其词向量模型也被公开供其他研究者使用\footnote{https://github.com/cbaziotis/ntua-slp-semeval2018}。

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{img/word2vec.png}
  \caption{Word2vec提供的两个算法的模型，引自\cite{mikolov2013efficient}}
  \label{fig:word2vec}
\end{figure}

基于矩阵分解的方法假设一个表示词和词或词和文档之间关系的矩阵可以表示为由词向量组成的矩阵和其他矩阵的相乘，因此对该关系矩阵以对应方式进行矩阵分解即可以得到词向量。具代表性的方法是隐藏语义分析方法（Latent semantic analysis，LSA）\cite{deerwester1990indexing}，以词在各个文档中出现的频率得出词-文档矩阵，并假设其文档可以表示为词-词概念、词概念-文档概念、文档概念-文档三个矩阵相乘，利用奇异值分解（Singular Value Decomposition，SVD）得到词-词概念矩阵，取每一行作为对应词的词向量。另外还有Pennington等人\cite{pennington2014glove}提出的GloVe算法，该算法根据不同词在语料中出现在各自上下文窗口中的频率得出词和词之间的关系矩阵，再分解成两个词-词概念矩阵的相乘。其研究结果显示在部分自然语言处理任务中，采用GloVe较Word2vec及另外几种词嵌入技术得出的词向量能达到更好的效果。他们同样公开了多个利用不同语料库训练好的词向量模型供其他研究者使用\footnote{https://nlp.stanford.edu/projects/glove/}。

\subsubsection{词汇特征} % Lexical Feature 

词汇特征表示文本中一元或多元语法的分布情况，不考虑词的具体意思。譬如常用的词袋模型（Bag of words, BOW），其将一段文本映射到 一个固定长度的向量，向量上每一维对应一个一元或多元语法的某种属性，譬如它是否在文本中出现（即独热编码），或它在文本中出现的频率，或其词频-逆文档频度(Term Frequency - Inverse Document Frequency，TF-IDF)。或更简地以单词数量或字母数量作为一维的特征。

\subsubsection{句法特征} % Syntactic features 

句法特征表示文本中各个词在句子的语法作用。常见做法是先对文本进行词性（Part-of-speech，POS）标注和依赖树分析，再基于标注提取特征，如形容词的数量、副词的数量，或以标注代替单词后采用词袋模型得出固定长度的特征向量。一些研究会针对性地手工设计句法特征，这要求研究者对语法的了解和挖掘它对研究的问题是否存在关联性，但特征的作用更直观，解释性更强。

\subsubsection{语义特征} % Semantic features

语义特征基于文本的字面意思，表示文本表达的内容或内容的个别属性‌（如主题分布），这更接近于人们透过理解句子内容来进行识别。常见技术譬如基于文本中各个词所对应词向量计算句子的表示向量，或利用词向量计算句子中各单词和情感词的相似度来间接评估词的情感极性，或利用聚类算法对语料库的单词进行聚类来获得单词的隐藏语义类别，再计算句子中各类别单词的分布。

\subsection{机器学習方法}
\label{ssec:model}

本小节将给出搭建情感识别系统相关的机器学習方法。目前用于情感识别的主流算法可以分成两大类：传统机器学习和人工神经网络。我们会分别对这两类算法中具代表性的例子给出介绍，然后再介绍集成学习方法，以在有限个模型的基础上搭建集成识别系统，达到比单个模型更高的性能。

\subsubsection{传统机器学习}

支持向量机（Support Vector Machine, SVM）\cite{cortes1995support} 由Cortes和Vapnik在1995年提出，是目前最常用的传统机器学习算法之一。在对二分类问题的建模过程中，每个训练样本被映射到一个空间中的一个点，而支持向量机尝试把找出一个边缘把空间分成两个子空间（如以直线分割二维空间，以平面分割三维空间），每一个子空间对应一个类，使得训练样本落在对应类别的子空间中，并且各样本到边缘的距离最大。对于多分类问题，主流做法是把原问题拆解成多个二分类问题。后随着核函数的引入，将支持向量机拓展成非线性的数学模型，大大加强了其建模能力。除了分类问题，支持向量机同样可以应用于回归问题，Drucker等人\cite{drucker1997support}则提出了支持向量机的回归模型。支持向量机最大的限制之一是只接受固定长度的向量作为输入，要求时先从输入数据提取出固定长度的特征向量，换言之基于支持向量机的识别系统非常依赖对特征的提取。

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{img/svm.pdf}
  \caption{支持向量机在处理二类问题的示情感。其中黑点和白点分别对应两个类别的样本，$L_1$不能对两类样本作区分，$L_2$和$L_3$均成功区分两个类别的样本，但$L_3$和两个类别样本的距离更大，为更优解}
  \label{fig:svm}
\end{figure}

决策树（Decision Tree）是另一种常见的机器学习方法。对于分类问题，决策树基于训练数据建立一个树状模型，其中每个非子叶结点对应一个决策，叶子结点对应要识别的类别。在识别阶段，识别过程从根结点开始，经过每个非叶子结点根据输入数据得出决策结果，不同的决策结果会对应下一级的一个结点，如此不断前进直到到达叶子结点，以其对应类别作为分类结果。和支持向量机一样，决策树的输入为固定长度的向量，要求对应的特征提取。现今的研究更多地会使用由多个决策树组成的随机森林（Random Forest）而非单个决策树作为分类器，随机森林属于集成学习一类，将在后续章节\ref{sssec:intro_ensemble_learning}中介绍。

\subsubsection{人工神经网络}

人工神经网络源自仿生学，以数学模型模拟生物神经网络的响应过程和学习过程。Rosenblatt提出的感知器\cite{rosenblatt1958perceptron}是最早的人工神经网络，也是现在人工神经网络的基本单元（又称为神经元）。Rumelhart等人\cite{rumelhart1985learning}在后来提出了一种多层前馈神经网络，BP网络，由多层的感知器和激励函数反复堆叠而成。更重要的是他们提出了它的学习算法，反向传播算法（Back-propagation，BP），当网络的输出结果和预期结果不同，透过构造损失函数计算偏差，再根据损失函数和偏差计算出神经元权重的梯度，其对应了使误差扩大的权重方向，以其反方向作为权重修改的依据。同时将该层输入的修改量作为前一层输出的偏差，逐层重复修改权重，整个过程重复迭代直到误差满足一定条件时学习过程结束。反向传播算法是目前人工神经网络模型训练中的核心算法，但随着网络层数增加，梯度以指数级别增大或缩小，会导致梯度大小过大或近似于零的情况，即梯度爆炸和梯度消失。直到Hinton等人\cite{hinton2006fast}提出深度信念网络和无监督逐层训练的策略，其核心思想是在每一轮训练只调整网络中一层的权重，透过固定前一层的输出作为本层的输入，避开了多次传播带来梯度的指数级变化。如此网络逐层经过调整后，再对整个网络进行权重调重，理论上此时的梯度较小，避免了梯度爆炸的出现。这使得现在复杂人工神经网络的学习成为了可能，深度学习正式进入高速发展的阶段。接下来我们将对主流的人工神经网络单元作出介绍。

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{img/perceptron.png}
  \caption{生物的神经元与感知器的模型示意图，引自\cite{basheer2000artificial}}
  \label{fig:perceptron}
\end{figure}

\begin{figure}[h]
  \centering%
  \subcaptionbox{Lecun等人\cite{lecun1989backpropagation}的反向传播网络
  \label{fig:lecun1989backpropagation}}
    {\includegraphics[width=0.42\textwidth]{img/lecun1989backpropagation.png}}%
  \hspace{4em}%
  \subcaptionbox{Waibel等人\cite{waibel1995phoneme}的时延神经网络\label{fig:waibel1995phoneme}}
      {\includegraphics[width=0.42\textwidth]{img/waibel1995phoneme.png}}
  \caption{卷积神经网络模型}
  \label{fig:cnn}
\end{figure}

卷积神经网络（Convolutional Neural Network，CNN）是目前最常用于计算机视觉的一类人工神经网络，它的设计来源自生物的视觉皮层。Hubel和Wiesel\cite{hubel1959receptive}\cite{hubel1968receptive}在对猫和猴子的视觉皮层进行观察时发现其中一些神经元只对接收到的视觉画面的一个区域作出反应，该区域称为该神经元的感受域（Receptive Field）而相邻神经元的感受域会有所重叠，联合构成对整个视觉画面的接收和反应。这些神经元中再细分为两类，第一类神经元对画面中特定方向的刺激反应最强烈，另二类神经元则较前一类神经元的感受域大，但其反应对刺激在感受域中的具体位置不敏感。

基于以上生物特性，Fukushima\cite{fukushima1980neocognitron}提出了神经认知机（Neocognitron），其数学模型中包含了两个关键结构，卷积层和降采样层，与前面两类神经元的功能一一对应。受启发于神经认知机，Lecun等人\cite{lecun1989backpropagation}提出了一个三层的反向传播网络，包含特征映射和降采样两种功能的元件，同样与视觉皮层中的两类神经元对应。该网络被应用于手写数字的识别，和当时同类型网络的性能相比达到了近30\%的提升，该数学模型自此成为了现今卷积神经网络的原型。

虽然卷积神经网络起源于视觉处理，但其变型还应用于其他领域。如Waibel等人\cite{waibel1995phoneme}提出的时延神经网络（Time-Delay Neural Network，TDNN）比应用于计算机听觉的音素识别，其核心思想是将每一个时刻的音频特征按时间顺序堆䟙，以时间上的相邻类比视觉画面上位置的相邻。Zhang等人\cite{zhang2015character}提出了一个字符级别的卷积神经网络并应用于文本主题分类，以字符类别语音中的信号，首次将卷积神经网络用于自然语言领域。

递归神经网络（Recurrent Neural Network，RNN）是另一个人工神经网络的大类，和前馈神经网络最大的区别在于递归神经网络中以某种机制实现了记忆能力来保留前一时刻的状态，因此可以对一个序列输入中前后出现的内容进行关联，换言之对序列的数据进行建模。递归神经网络会在每一时刻接受一个输入，并结合其“记忆”计算这一时刻的输出，故递归神经网络的输出是和输入序列长度相同的序列。递归神经网络的应用场景包括视频内容识别、文本翻译、股票指标预测等。较早期被提出的递归神经网络模型是Elman\cite{elman1990finding}的Elman网络，其数学模型如下：

\begin{align}
  \label{eq:elman_rnn}
  h_t &= \sigma_h (W_h x_t + U_h h_{t-1} + b_h) \\
  y_t &= \sigma_y (Y_y h_t + b_y)
\end{align}

其中$x_t \in \mathbb{R}^d $为$t$时刻的输入，$h_t, h_{t-1} \in \mathbb{R}^h $分别表示网络在这一时刻和上一时刻的隐藏状态，$y_t \in \mathbb{R}^y$为网络的输出，$\sigma_h$和$\sigma_y$为激活函数，$W_h \in \mathbb{R}^{h \times d}, U_h \in \mathbb{R} ^ {h \times h}, b_h \in \mathbb{R}^h, Y_h \in \mathbb{R}^{y \times h}, b_y \in \mathbb{R}^y$为模型参数。可见在模型中因为引入$h_{t-1}$作为当前时刻的输入而表现出某种记忆，上一时刻的输入$x_{t-1}$间接影响了这一轮的输出。

目前常用的递归神经网络之一是Hochreiter和Schmidhuber\cite{hochreiter1997long}提出的长短时记忆模型（Long Short-Term Memory，LSTM），其数学模型如下：

\begin{align}
  \label{eq:lstm}
  f_t &= \sigma_g(W_f x_t + U_f h_{t - 1} + b_f) \\
  i_t &= \sigma_g(W_i x_t + U_i h_{t - 1} + b_i) \\
  o_t &= \sigma_g(W_o x_t + U_o h_{t - 1} + b_o) \\
  c_t &= f_t \circ c_{t-1} + i_t \circ \sigma_c (W_c x_t + U_c h_{t-1} + b_c) \\
  h_t &= o_t \circ \sigma_h (c_t)
\end{align}

其中$x_t \in \mathbb{R}^d$ 为$t$时刻的输入，$h_t$，$h_{t-1} \in \mathbb{R}^h$分别是这一时刻和上一时刻LSTM的输出（或当LSTM作为一个人工神经网络中其中一层时称为隐藏状态），
$c_t \in \mathbb{R}^h$提当前时刻记忆，$i_t$，$f_t$，$o_t \in \mathbb{R}^h$分别控制了
这一时刻的输入、上一时刻记忆和上一时刻的输出在这一刻的起的作用，因此称为“输入门”、“遗忘门”、“输出门”的激活向量，$\sigma_g, \sigma_c, \sigma_h$为激活函数，特别地$\sigma_g$一般采用 $sigmoid$函数，$\sigma_c, \sigma_h$一般采用$tanh$函数，$W \in \mathbb{R} ^ {h \times d}, U \in \mathbb{R}^{h \times h}, b \in \mathbb{R} ^ {h}$为网络参数。和Elman网络比较可见，LSTM除了有$h_{t-1}$传递上一时刻的信息以外多了一个记忆单元$c$，具有更强的记忆功能。在LSTM的设计中包含了“输入门”、“遗忘门”、“输出门”和记忆单元，对序列数据有较强拟合能力，相对地对于小规模的序列数据容易出现过拟合。

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{img/lstm.png}
  \caption{长短时记忆模型，引自\cite{graves2013hybrid}}
  \label{fig:lstm}
\end{figure}

为了解决容现出现过拟合的问题，Cho等人\cite{cho2014learning}提出了门控循环神经元（Gated Recurrent Unit, GRU）。可以认为它是LSTM的一种轻量级变型，其数学模型如下：

\begin{align}
  \label{eq:gru}
  z_t &= \sigma_g (W_z x_t + U_z h_{t-1} + b_z) \\
  r_t &= \sigma_g (W_r x_t + U_r h_{t-1} + g_r) \\
  h_t &= (1 - z_t) \circ h_{t-1} + z_t \circ \sigma_h (W_h x_t + U_h (r_t \circ h_{t-1}) + b_h) 
\end{align}

可见在数学模型上GRU和LSTM非常想似，虽然GRU中省去了专门的记忆单元，但同样采用了“门”来控制当前时刻的输入和前一时刻输出的作用。GRU的模型结构要较LSTM的简单，而在同一组超参数下，GRU的模型参数要明显少于LSTM，故理论上在处理较小规模的序列数据时更有优势。

从上面的数学模型中我们不难发现，时刻$t$的输出只受时刻$t$及以前的输入影响，不过在一些场景下我们会考虑时刻$t$以后的输入是会也会与时刻$t$的输出有关。譬如在文本情感识别时，英语有一种后置定语的语法，在句子中一个单词A由出现在它后面的一组单词来修饰，那么为了正确理解在时刻$t$出现的单词A，我们就需要引入时刻$t$以后的信息。为此Schuster和Paliwal\cite{schuster1997bidirectional}提出了双向递归神经网络（Bidirectional Recurrent Neural Network，BRNN），如图\ref{fig:brnn}所示，一个双向递归神经网络包含两个RNN，对于一组序列数据，分别以其原本方向和反方向输入到两个RNN中，从而获得两个方向上与$t$时刻相关的信息。根据此思想，RNN可以替换成任何具体的递归神经网络如LSTM和GRU。

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{img/brnn.png}
  \caption{双向递归神经网络，引自\cite{schuster1997bidirectional}}
  \label{fig:brnn}
\end{figure}

注意力机制（Attention Mechanism）受启发于人类的视觉处理机制，当人们尝试理解一个画面的内容时，他们不会马上对画面里的所有细节都进行处理，而是首先根据需要找出相关信息所在的部位，再对该部分的内容佢进一步处理。当人们对同类型画面有一定观察后，会总结出关键信息出现的模式，并在处理新的画面时优先采用这些模式来理解其中的内容。Mnih等人\cite{mnih2014recurrent}就提出了结合递归神经网络和注意力机制来实现对手写数字的识别，不是透过预处理而是直接依赖注意力机制找出图片中数字所在的位置。其后Bahdanau等人\cite{bahdanau2014neural}同样尝试了在递归神经网络中加入注意力机制，但这次的应用场景从计算机视觉转移到了自然语言处理的机器翻译，其注意力机制的模型如下：

\begin{align}
  \label{eq:attention}
  e_{ij} &= a(s_{i - 1}, h_j) \\
  \alpha_{ij} &= \frac{exp(e_{ij})}{\sum\limits_{k=1}^{T_x}{exp(e_{ik})}} \\
  c_{i} &= \sum\limits_{j=1}^{T_x}{\alpha_{ij}h_j}
\end{align}

其中$s_{i-1}$表示上一时刻（$t-1$）的隐藏状态，$h_j$表示在时刻$j$的输入信息，a是一个函数以计算$s_{i - 1}$和$h_j$关联性，$\alpha_{ij} \in (0, 1]$表示对输出时刻$i$，给予输入时刻$j$的信息的注意力，最后根据各个时刻应该给予的注意力结合所有输入信息，得出用于时刻$i$输出的浓缩信息$c_{i}$。其后Vaswani等人\cite{vaswani2017attention}则仅使用注意力机制搭建的人工神经模型在机器翻译领域达到了引人注目的性能。虽然在近年不同研究\cite{luong2015effective}\cite{graves2014neural}\cite{xu2015show}中同现了各种注意力机制的变型，但这些模型可以统一描述如下：

\begin{align}
  \alpha_{ij} &= a(q_i, k_j) \\
  c_i &= \frac
    {\sum\limits_{j=1}^{T_x}{\alpha_{ij}v_j}}
    {\sum\limits_{j=1}^{T_x}{\alpha_{ij}}}
\end{align}

其中$q_i$表示输出时刻$i$对应的一个请求，$k_j$和$v_j$表示输入时刻$j$的一个键值对，a是一个数计算请求和键之间关联性，以计算对于输出时刻$i$应给予输入时刻$j$的信息$v_j$的注意力$\alpha_{ij}$，最后以加权平均计算出输出时刻$i$相关的背景信息$c_i$。一些研究\cite{wang2016attention}\cite{pontiki2014semeval}则针对其中的隐藏状态$\alpha_{ij}$分析注意力机制的具体运行，并得出了和人工理解一致的结果。

\subsubsection{集成学习}
\label{sssec:intro_ensemble_learning}

集成学习（Ensemble Learning）的目的在于结合多个模型以达到比其中任何一个模型都要好的识别性能。集成学习方法包含了多个类别，其中常用于分类问题的是基于投票的方法，譬如多数投票（Majority Voting）、加权多数投票（Weighted Majority Vote）、加权平均概率投票（Soft Voting）。假设对于$N$分类问题，有$T$个基础模型$h_i, i \in [1, T]$，对于输入数据$x$，$h_i(x) \in \{0, 1\}^N$为各个模型的分类结果。对于多数投票其最终预测结果$H_M(x)$如下：

\begin{align}
  H_{M}(x) &= \frac{1}{T} \sum\limits_{i=1}^{T}h_i(x)
\end{align}

对于加权多数投票，其最终预测结果$H_{WM}(x)$如下：

\begin{align}
  H_{WM}(x) &= \sum\limits_{i=1}^{T}w_i h_i(x)
\end{align}

其中$\sum\limits_{i=1}^{T}w_i = 1, w_i \geq 0$为每个模型的权重，相当于假设不同模型具有不同的可信度。而加权平均概率投票要求模型能给出样本在不同类别上的概率分布$p_i(x) \in [0, 1]^N$，其最终预测概率分布$P_S(x)$如下：

\begin{align}
  P_{S}(x) &= \sum\limits_{i=1}^{T}w_i p_i(x)
\end{align}

其中$\sum\limits_{i=1}^{T}w_i = 1, w_i \geq 0$，和加权多数投票中$w_i$的原理相同，最后取概率最高的类别作为最终分类结果。

\section{本章小结}

在本章我们首先对情感识别给出统一的形式化表示，引出了其中涉及的各个元素并描述他们的相互关系。然后描述了我们面向情感识别的研究框架，其中包括了两个研究重点。一是研究以不同算法得到的分类器对研究问题的整体性能，并作深入的分析，包括单个算法对不同类别样本的识别能力、比较不同算法在不同指标上的区别以理解他们的拟合倾向、观察被错误识别的样本并找出其原因等。二是研究基于基础分类器的集成系统，鉴于单个分类器的识别能力有限，我们提出了一个集成系统的设计方案，旨在结合多个基础分类器的预测结果来得出最终的判断，从而充分发挥一个数学模型的拟合能力，细节实现方法将在后续具体的应用场景中给出说明。



